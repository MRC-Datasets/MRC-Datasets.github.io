---
layout: default
---

  <div class="section no-pad-bot" id="index-banner">
    <div class="container">
      <br><br>
      <h1 class="header center orange-text">{{ site.title }}</h1>
      <div class="row center">
       <h5 class="header col s12 light">{{ content }}</h5>
      </div>
        
      <div class="row center">
        <a href="../data/MRC_Datasets_Statistics.xlsx"  target="_blank" id="download-button" class="btn-large waves-effect waves-light orange">Data Download</a>
      </div>
      <br><br>

    </div>
  </div>

  <div class="container">
    <div class="section">

      <!--   Icon Section   -->
      <div class="row">
        <div class="col s12 m4">
          <div class="icon-block">
						
            <h2 class="center light-blue-text"><a href="https://mrc-datasets.github.io/tasks/"><i class="material-icons" >build</i></a></h2>
            <h5 class="center"><a href="https://mrc-datasets.github.io/metrics/">MRC Tasks</a></h5>		
						
						<p class="light">In order to have a better understanding of the existing MRC tasks, we analyzed the existing classification method of tasks and pointed out the limitations of this method. After analyzing 57 MRC tasks and datasets, we propose a more precise classification method of MRC tasks which has 4 different attributes of MRC task and each of them could be divided into several types, and the statistical data of 57 MRC tasks are also given in this section.</p>
          </div>
        </div>

        <div class="col s12 m4">
          <div class="icon-block">
            <h2 class="center light-blue-text"><a href="https://mrc-datasets.github.io/datasets/"><i class="material-icons">cloud_queue</i></a></h2>
            <h5 class="center"><a href="https://mrc-datasets.github.io/datasets/">MRC Datasets</a></h5>

            <p class="light">The recent success of neural reading comprehension is driven by both large-scale datasets and neural models. In this section, we analyzed the attributes of different MRC benchmark datasets, including: the size of the dataset, the generation method of datasets, the source of corpus, the type of context, the availability of leaderboards and baselines, reasoning skills and citations of related papers. </p>
          </div>
        </div>

        <div class="col s12 m4">
          <div class="icon-block">
            <h2 class="center light-blue-text"><a href="https://mrc-datasets.github.io/metrics/"><i class="material-icons">check_box</i></a></h2>
            <h5 class="center"><a href="https://mrc-datasets.github.io/metrics/">MRC Metrics</a></h5>

            <p class="light">In this section, we introduce the computation methods of MRC evaluation metrics. Typical evaluation metrics of MRC datasets are: Accuracy, Exact Match, F1 score, ROUGE, BLEU, HEQ and Meteor. Many datasets use more than one evaluation metric. Moreover, some datasets adopt detailed evaluation metrics according to their own characteristics,such as Accuracy on Named Entities, F1 of Supportings, etc. </p>
          </div>
        </div>
      </div>

    </div>
      
    </div>
    <div style="background: #ddd">
    <div class="container last-post">
    <section>
        <h5>Latest Post</h5>
        <ul class="collection">
        {% for post in site.posts %}
        <li class="collection-item avatar">
          {% assign date_format = site.minima.date_format | default: "%m/%d" %}
          <div class="date-post">{{ post.date | date: date_format }}</div>
          <span class="title"><a class="post-link" href="{{ post.url | relative_url }}">{{ post.title | escape }}</a></span>
          <p>
             {{ post.content | truncatewords: 10 }}
          </p>
          <a href="{{ post.url | relative_url }}" class="secondary-content"><i class="material-icons">navigate_next</i></a>
        </li>
        {% endfor %}
        </ul>  
    </section>
    </div>
</div>

  </div>
